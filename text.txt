MLP

# %%
import numpy as np

class MLP:
    def __init__(self, input_size, hidden_size, output_size, learning_rate):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.learning_rate = learning_rate

        # Initialize weights and biases for hidden and output layers
        self.hidden_weights = np.random.randn(self.input_size, self.hidden_size)
        self.hidden_weights1 = np.round(self.hidden_weights, 1)

        self.hidden_bias = np.random.randn(self.hidden_size)
        self.hidden_bias1 = np.round(self.hidden_bias, 1)

        self.output_weights = np.random.randn(self.hidden_size, self.output_size)
        self.output_weights1 = np.round(self.output_weights, 1)

        self.output_bias = np.random.randn(self.output_size)
        self.output_bias1 = np.round(self.output_bias, 1)

    def show(self):
      print(self.hidden_weights1)
      print(self.hidden_bias1)
      print(self.output_weights1)
      print(self.output_bias1)

    def forward(self, X):
        # Compute hidden layer output
        self.hidden_output = np.dot(X, self.hidden_weights1) + self.hidden_bias1
        self.hidden_activation = self.sigmoid(self.hidden_output)

        # Compute output layer output
        self.output = np.dot(self.hidden_activation, self.output_weights1) + self.output_bias1
        self.output_activation = self.sigmoid(self.output)

        return self.output_activation

    def backward(self, X, y, output):
        # Compute error and delta for output layer
        error = y - output
        output_delta = error * self.sigmoid_derivative(output)

        # Compute error and delta for hidden layer
        hidden_error = np.dot(output_delta, self.output_weights1.T)
        hidden_delta = hidden_error * self.sigmoid_derivative(self.hidden_activation)

        # Update weights and biases for output and hidden layers
        self.output_weights1 += self.learning_rate * np.dot(self.hidden_activation.T, output_delta)
        self.output_bias1 += self.learning_rate * np.sum(output_delta, axis=0)
        self.hidden_weights1 += self.learning_rate * np.dot(X.T, hidden_delta)
        self.hidden_bias1 += self.learning_rate * np.sum(hidden_delta, axis=0)

    def train(self, X, y, epochs):
        for i in range(epochs):
            # Forward pass
            output = self.forward(X)

            # Backward pass
            self.backward(X, y, output)

            # Print loss for every 1000 epochs
            loss = np.mean(np.square(y - output))
            print(f"Epoch {i}, Loss: {loss:.4f}")

    def predict(self, X):
        return self.forward(X)

    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))

    def sigmoid_derivative(self, x):
        return x * (1 - x)

# %%
# Generate random input and output data
X = np.random.randn(2, 3)
X1 = np.round(X, 1)

y = np.random.randn(2, 1)
y1 = np.round(y, 1)

# %%
# Create MLP with input size of 4, hidden size of 5, output size of 1, and learning rate of 0.1
mlp = MLP(3, 4, 1, 0.1)

# %%
mlp.show()

# %%
mlp.train(X1, y1, 10)

# %%
mlp.show()

# %%
X1

# %%
y1


Perceptron

# %%
# evaluate a perceptron model on the dataset
from numpy import mean
from numpy import std
from sklearn.datasets import make_classification
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.linear_model import Perceptron

# %%
# define dataset
X, y = make_classification(n_samples=1000, n_features=10, n_informative=10, n_redundant=0, random_state=1)

print(X.shape, y.shape)

# %%
# define model
model = Perceptron()

# define model evaluation method
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)

# evaluate model
scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)

# summarize result
print('Mean Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))

# %%
# fit model
model.fit(X, y)

# define new data
row = [0.12777556,-3.64400522,-2.23268854,-1.82114386,1.75466361,0.1243966,1.03397657,2.35822076,1.01001752,0.56768485]

# make a prediction
yhat = model.predict([row])

# summarize prediction
print('Predicted Class: %d' % yhat)

# %%
from sklearn.model_selection import GridSearchCV

# define grid
grid = dict()
grid['eta0'] = [0.0001, 0.001, 0.01, 0.1, 1.0]

# define search
search = GridSearchCV(model, grid, scoring='accuracy', cv=cv, n_jobs=-1)

# perform the search
results = search.fit(X, y)

# summarize
print('Mean Accuracy: %.3f' % results.best_score_)
print('Config: %s' % results.best_params_)

# summarize all
means = results.cv_results_['mean_test_score']
params = results.cv_results_['params']
for mean, param in zip(means, params):
    print(">%.3f with: %r" % (mean, param))

# %%
# define model
model = Perceptron(eta0=0.0001)

# define grid
grid = dict()
grid['max_iter'] = [1, 10, 100, 1000, 10000]

# define search
search = GridSearchCV(model, grid, scoring='accuracy', cv=cv, n_jobs=-1)

# perform the search
results = search.fit(X, y)

# summarize
print('Mean Accuracy: %.3f' % results.best_score_)
print('Config: %s' % results.best_params_)

# summarize all
means = results.cv_results_['mean_test_score']
params = results.cv_results_['params']
for mean, param in zip(means, params):
    print(">%.3f with: %r" % (mean, param))

# %%
# define model
model = Perceptron(eta0=0.0001, max_iter=10)

# fit model
model.fit(X, y)

# define new data
row = [0.12777556,-3.64400522,-2.23268854,-1.82114386,1.75466361,0.1243966,1.03397657,2.35822076,1.01001752,0.56768485]

# make a prediction
yhat = model.predict([row])

# summarize prediction
print('Predicted Class: %d' % yhat)


IRIs

# %%
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris

# %%
# Load the Iris dataset
iris = load_iris()

# %%
# We will only use the first two features (sepal length and sepal width) and the first 100 samples
X = iris.data[:100, :2]
y = iris.target[:100]

# %%
# Define the perceptron function
def perceptron(X, y, learning_rate=0.1, epochs=100):
    # Initialize the weight vector to zeros
    w = np.zeros(X.shape[1])
    # Initialize the bias to zero
    b = 0
    # Loop over the specified number of epochs
    for epoch in range(epochs):
        # Loop over each sample in the dataset
        for i in range(X.shape[0]):
            # Calculate the predicted class label
            y_pred = np.dot(w, X[i]) + b
            if y_pred > 0:
                y_pred = 1
            else:
                y_pred = 0
            # Update the weights and bias if the predicted class label does not match the true class label
            if y[i] != y_pred:
                w += learning_rate * (y[i] - y_pred) * X[i]
                b += learning_rate * (y[i] - y_pred)
    # Return the weight vector and bias
    return w, b

# %%
# Run the perceptron function
w, b = perceptron(X, y)

# %%
# Create a meshgrid of points to visualize the decision boundary
xx, yy = np.meshgrid(np.arange(X[:, 0].min() - 1, X[:, 0].max() + 1, 0.01),
                     np.arange(X[:, 1].min() - 1, X[:, 1].max() + 1, 0.01))
Z = np.dot(np.c_[xx.ravel(), yy.ravel()], w) + b
Z = np.where(Z > 0, 1, 0)
Z = Z.reshape(xx.shape)

# %%
# Plot the data points and decision boundary
plt.contourf(xx, yy, Z, alpha=0.5)
plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k')
plt.xlabel('Sepal length')
plt.ylabel('Sepal width')
plt.title('Perceptron learning algorithm')
plt.show()


